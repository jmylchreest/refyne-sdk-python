"""API types for the Refyne SDK.

These types are generated from the OpenAPI specification.
Do not edit this file manually - run `python -m refyne.generate` to regenerate.
"""

from __future__ import annotations

from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Literal


# ============================================================================
# Request Types
# ============================================================================


@dataclass
class LlmConfig:
    """LLM provider configuration for extraction.

    Attributes:
        provider: LLM provider name
        api_key: API key for the provider (BYOK)
        base_url: Custom base URL (for Ollama or self-hosted)
        model: Model to use
    """

    provider: Literal["anthropic", "openai", "openrouter", "ollama", "credits"] | None = None
    api_key: str | None = None
    base_url: str | None = None
    model: str | None = None


@dataclass
class ExtractRequest:
    """Request body for data extraction.

    Attributes:
        url: URL to extract data from
        schema: Schema defining the data structure to extract
        fetch_mode: Fetch mode (auto, static, or dynamic)
        llm_config: Custom LLM configuration
    """

    url: str
    schema: dict[str, Any]
    fetch_mode: Literal["auto", "static", "dynamic"] | None = None
    llm_config: LlmConfig | None = None


@dataclass
class CrawlOptions:
    """Options for crawl jobs.

    Attributes:
        follow_selector: CSS selector for links to follow
        follow_pattern: Regex pattern for URLs to follow
        max_depth: Maximum crawl depth (default: 1, max: 5)
        next_selector: CSS selector for pagination next link
        max_pages: Maximum pages to crawl (default: 10, max: 100)
        max_urls: Maximum total URLs to process (default: 50, max: 500)
        delay: Delay between requests (default: "500ms")
        concurrency: Concurrent requests (default: 3, max: 10)
        same_domain_only: Only follow same-domain links (default: true)
        extract_from_seeds: Extract data from seed URLs
    """

    follow_selector: str | None = None
    follow_pattern: str | None = None
    max_depth: int | None = None
    next_selector: str | None = None
    max_pages: int | None = None
    max_urls: int | None = None
    delay: str | None = None
    concurrency: int | None = None
    same_domain_only: bool | None = None
    extract_from_seeds: bool | None = None


@dataclass
class CrawlRequest:
    """Request body for starting a crawl job.

    Attributes:
        url: Seed URL to start crawling from
        schema: Schema defining the data structure to extract
        options: Crawl options
        webhook_url: Webhook URL to receive completion notification
        llm_config: Custom LLM configuration
    """

    url: str
    schema: dict[str, Any]
    options: CrawlOptions | None = None
    webhook_url: str | None = None
    llm_config: LlmConfig | None = None


@dataclass
class AnalyzeRequest:
    """Request body for website analysis.

    Attributes:
        url: URL to analyze
        depth: Analysis depth (default: 1)
    """

    url: str
    depth: int | None = None


@dataclass
class CreateSchemaRequest:
    """Request body for creating a schema.

    Attributes:
        name: Schema name
        description: Schema description
        schema_yaml: Schema definition in YAML format
        category: Category for organization
    """

    name: str
    schema_yaml: str
    description: str | None = None
    category: str | None = None


@dataclass
class CreateSiteRequest:
    """Request body for creating a saved site.

    Attributes:
        name: Site name
        url: Site URL
        schema_id: Associated schema ID
        crawl_options: Default crawl options
    """

    name: str
    url: str
    schema_id: str | None = None
    crawl_options: CrawlOptions | None = None


@dataclass
class CreateApiKeyRequest:
    """Request body for creating an API key.

    Attributes:
        name: Name for the API key
    """

    name: str


@dataclass
class UpsertLlmKeyRequest:
    """Request body for upserting an LLM provider key.

    Attributes:
        provider: Provider name
        api_key: API key
        default_model: Default model for this provider
        base_url: Custom base URL
        is_enabled: Whether the key is enabled
    """

    provider: Literal["anthropic", "openai", "openrouter", "ollama"]
    api_key: str
    default_model: str
    base_url: str | None = None
    is_enabled: bool | None = None


@dataclass
class LlmChainEntry:
    """Entry in the LLM fallback chain.

    Attributes:
        provider: Provider name
        model: Model name
        id: Entry ID (optional, server-generated)
        position: Position in chain (optional, server-generated)
        is_enabled: Whether this entry is enabled
    """

    provider: str
    model: str
    id: str | None = None
    position: int | None = None
    is_enabled: bool | None = None


@dataclass
class SetLlmChainRequest:
    """Request body for setting the fallback chain.

    Attributes:
        chain: Ordered list of fallback entries
    """

    chain: list[LlmChainEntry]


# ============================================================================
# Response Types
# ============================================================================


@dataclass
class TokenUsage:
    """Token usage information from extraction.

    Attributes:
        input_tokens: Number of input tokens used
        output_tokens: Number of output tokens used
        cost_usd: Total USD cost charged
        llm_cost_usd: Actual LLM cost from provider
        is_byok: True if user's own API key was used
    """

    input_tokens: int
    output_tokens: int
    cost_usd: float
    llm_cost_usd: float
    is_byok: bool


@dataclass
class ExtractionMetadata:
    """Metadata from extraction.

    Attributes:
        fetch_duration_ms: Time to fetch the page in milliseconds
        extract_duration_ms: Time to extract data in milliseconds
        model: Model used for extraction
        provider: LLM provider used
    """

    fetch_duration_ms: int
    extract_duration_ms: int
    model: str
    provider: str


@dataclass
class ExtractResponse:
    """Response from data extraction.

    Attributes:
        data: Extracted data matching the schema
        url: URL that was extracted
        fetched_at: Timestamp when the page was fetched
        usage: Token usage information
        metadata: Extraction metadata
    """

    data: dict[str, Any]
    url: str
    fetched_at: str
    usage: TokenUsage | None = None
    metadata: ExtractionMetadata | None = None


class JobStatus(str, Enum):
    """Job status enum."""

    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"


@dataclass
class CrawlJobCreated:
    """Response when a crawl job is created.

    Attributes:
        job_id: Unique job identifier
        status: Initial job status
        status_url: URL to check job status
    """

    job_id: str
    status: JobStatus
    status_url: str


@dataclass
class Job:
    """Job details.

    Attributes:
        id: Job ID
        type: Job type
        status: Current status
        url: Seed URL
        urls_queued: Number of URLs queued
        page_count: Number of pages processed
        token_usage_input: Input tokens used
        token_usage_output: Output tokens used
        cost_usd: Cost in USD
        error_message: Error message if failed
        started_at: When the job started
        completed_at: When the job completed
        created_at: When the job was created
    """

    id: str
    type: str
    status: JobStatus
    url: str
    urls_queued: int
    page_count: int
    token_usage_input: int
    token_usage_output: int
    cost_usd: float
    created_at: str
    error_message: str | None = None
    started_at: str | None = None
    completed_at: str | None = None


@dataclass
class JobList:
    """List of jobs response.

    Attributes:
        jobs: List of jobs
    """

    jobs: list[Job]


@dataclass
class JobResults:
    """Job results response.

    Attributes:
        job_id: Job ID
        status: Job status
        page_count: Number of pages processed
        results: Array of extraction results (when merge=false)
        merged: Merged results object (when merge=true)
    """

    job_id: str
    status: JobStatus
    page_count: int
    results: list[dict[str, Any]] | None = None
    merged: dict[str, Any] | None = None


@dataclass
class AnalyzeResponse:
    """Response from website analysis.

    Attributes:
        url: URL that was analyzed
        suggested_schema: Suggested schema based on page content
        follow_patterns: Suggested URL patterns for crawling
    """

    url: str
    suggested_schema: dict[str, Any]
    follow_patterns: list[str]


@dataclass
class Schema:
    """Schema definition.

    Attributes:
        id: Schema ID
        name: Schema name
        schema_yaml: Schema definition in YAML
        created_at: Creation timestamp
        updated_at: Last update timestamp
        description: Schema description
        category: Category
    """

    id: str
    name: str
    schema_yaml: str
    created_at: str
    updated_at: str
    description: str | None = None
    category: str | None = None


@dataclass
class SchemaList:
    """List of schemas.

    Attributes:
        schemas: List of schemas
    """

    schemas: list[Schema]


@dataclass
class Site:
    """Saved site configuration.

    Attributes:
        id: Site ID
        name: Site name
        url: Site URL
        created_at: Creation timestamp
        schema_id: Associated schema ID
        crawl_options: Default crawl options
    """

    id: str
    name: str
    url: str
    created_at: str
    schema_id: str | None = None
    crawl_options: CrawlOptions | None = None


@dataclass
class SiteList:
    """List of saved sites.

    Attributes:
        sites: List of sites
    """

    sites: list[Site]


@dataclass
class ApiKey:
    """API key (without the secret).

    Attributes:
        id: Key ID
        name: Key name
        prefix: Key prefix for identification
        created_at: Creation timestamp
        last_used_at: Last used timestamp
    """

    id: str
    name: str
    prefix: str
    created_at: str
    last_used_at: str | None = None


@dataclass
class ApiKeyCreated:
    """Newly created API key (includes the full key).

    Attributes:
        id: Key ID
        name: Key name
        key: Full API key - only shown once
    """

    id: str
    name: str
    key: str


@dataclass
class ApiKeyList:
    """List of API keys.

    Attributes:
        keys: List of keys
    """

    keys: list[ApiKey]


@dataclass
class UsageResponse:
    """Usage statistics.

    Attributes:
        total_jobs: Total number of jobs
        total_charged_usd: Total USD charged for usage
        byok_jobs: Jobs using user's own API keys (not charged)
    """

    total_jobs: int
    total_charged_usd: float
    byok_jobs: int


@dataclass
class LlmKey:
    """LLM provider key configuration.

    Attributes:
        id: Key ID
        provider: Provider name
        default_model: Default model
        is_enabled: Whether the key is enabled
        created_at: Creation timestamp
        base_url: Custom base URL
    """

    id: str
    provider: str
    default_model: str
    is_enabled: bool
    created_at: str
    base_url: str | None = None


@dataclass
class LlmKeyList:
    """List of LLM provider keys.

    Attributes:
        keys: List of keys
    """

    keys: list[LlmKey]


@dataclass
class LlmChain:
    """LLM fallback chain.

    Attributes:
        chain: Ordered chain entries
    """

    chain: list[LlmChainEntry]


@dataclass
class Model:
    """Available model.

    Attributes:
        id: Model ID
        name: Model display name
    """

    id: str
    name: str


@dataclass
class ModelList:
    """List of available models.

    Attributes:
        models: List of models
    """

    models: list[Model]
